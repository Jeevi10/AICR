{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#from pushover import notify\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "import dataloader as dl\n",
    "import model as m\n",
    "import networks\n",
    "from networks import LeNet, ClassificationNet\n",
    "from testers import attack_test\n",
    "from resnet import ResNet\n",
    "import gmm as gmm\n",
    "import parameters as p\n",
    "import helper\n",
    "import misc\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256\n",
    "train_loader,test_loader,loader_list = misc.get_dataloaders(\"Lenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_x, _ = next(iter(loader_list[0]))\n",
    "save_image(fixed_x, 'real_image.png')\n",
    "\n",
    "Image('real_image.png')\n",
    "\n",
    "\n",
    "def in_top_k(targets, preds, k):\n",
    "    topk = preds.topk(k,largest=False)[1]\n",
    "    return (targets.unsqueeze(1) == topk).any(dim=1)\n",
    "\n",
    "\n",
    "def cross_corr(centers):\n",
    "    c = centers.view(-1,10*centers.size(1))\n",
    "    corr =torch.matmul(c.T,c)\n",
    "    loss = torch.norm(torch.triu(corr, diagonal=1, out=None))\n",
    "    return 2*loss/corr.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proximity(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=100, feat_dim=1024, use_gpu=True, margin = 0.0 ):\n",
    "        super(Proximity, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.use_gpu = use_gpu\n",
    "        self.device = torch.device(\"cuda:1\")\n",
    "        self.margin = margin\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.centers =  nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x , labels):\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: classes = classes.to(self.device)\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "        d_y = distmat[mask.clone()]\n",
    "        \n",
    "        \n",
    "        values, indices = torch.topk(distmat,2, dim=1, largest=False, sorted=True, out=None)\n",
    "        d_1 = values[:,0]\n",
    "        d_2 = values[:,1]\n",
    "        \n",
    "        indicators = in_top_k(labels,distmat,1)[:,0]\n",
    "        con_indicators = ~ indicators.clone()\n",
    "        \n",
    "        d_c = d_2*indicators + d_1*con_indicators\n",
    "        \n",
    "        loss = F.relu((d_y-d_c)/(d_y+d_c) + self.margin)\n",
    "        mean_loss = loss.mean()\n",
    "        return mean_loss, torch.argmin(distmat,dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_net = LeNet()\n",
    "model = ClassificationNet(embedding_net, n_classes=p.n_classes).cuda()\n",
    "gmm = gmm.GaussianMixturePrior(p.num_classes, network_weights=list(model.embedding_net.layers.parameters()), pi_zero=0.99).cuda()\n",
    "                                       \n",
    "criterion_prox_256 = Proximity(num_classes=10, feat_dim=256, use_gpu=True,margin=0.75)\n",
    "criterion_prox_1024 = Proximity(num_classes=10, feat_dim=1024, use_gpu=True, margin=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_pre = torch.optim.Adam([{'params':model.parameters()}], lr=1e-3, weight_decay=1e-7)\n",
    "#optimizer_post = torch.optim.Adam([{'params':model.parameters()},\n",
    "#                                 {'params': gmm.means, 'lr': p.lr_mu},\n",
    "#                                 {'params': gmm.gammas, 'lr': p.lr_gamma},\n",
    "#                                 {'params': gmm.rhos, 'lr': p.lr_rho}], lr=p.lr_post)\n",
    "optimizer_post = torch.optim.Adam([{'params':model.parameters()}], lr=5e-3, weight_decay=1e-7)\n",
    "#optimizer_prox_1024 = torch.optim.SGD(criterion_prox_1024.parameters(), lr=0.1)\n",
    "#optimizer_conprox_1024 = torch.optim.SGD(criterion_conprox_1024.parameters(), lr=0.0001)\n",
    "                                         \n",
    "                                         \n",
    "optimizer_prox_256 = torch.optim.SGD(criterion_prox_256.parameters(), lr=0.01)\n",
    "optimizer_prox_1024 = torch.optim.SGD(criterion_prox_1024.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "criterion =  nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rfr reconstructed\n",
    "!rm -rfr softmaxreconstructed\n",
    "!rm -rfr figs\n",
    "!mkdir reconstructed\n",
    "!mkdir softmaxreconstructed\n",
    "!mkdir figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_0 = 50\n",
    "epochs_1 = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.patheffects as PathEffects\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "RS = 123\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
    "              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "              '#bcbd22', '#17becf']\n",
    "\n",
    "mnist_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "def t_sne_gen(data):\n",
    "    fashion_tsne = TSNE(random_state=RS).fit_transform(data.numpy())\n",
    "    #fashion_pca = PCA(n_components=2, svd_solver='full').fit(data.numpy())\n",
    "    #x = fashion_pca.transform(data.numpy())\n",
    "    return fashion_tsne\n",
    "\n",
    "\n",
    "def fashion_scatter(x, colors,name,folder):\n",
    "    # choose a color palette with seaborn.\n",
    "    num_classes = len(np.unique(colors))\n",
    "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
    "    plt.title(name)\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # add the labels for each digit corresponding to the label\n",
    "    txts = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "\n",
    "        # Position of each label at median of data points.\n",
    "\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "   \n",
    "    plt.savefig(folder+name+'.png')\n",
    "\n",
    "    return f, ax, sc, txts\n",
    "\n",
    "\n",
    "def plot_embeddings(embeddings, targets, xlim=None, ylim=None):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(10):\n",
    "        #ax = fig.add_subplot(111, projection='3d')\n",
    "        inds = np.where(targets==i)[0]\n",
    "        ax.scatter(embeddings[inds,0], embeddings[inds,1], embeddings[inds,2], alpha=0.5, color=colors[i])\n",
    "    if xlim:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    if ylim:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    plt.legend(mnist_classes)\n",
    "\n",
    "def extract_embeddings(dataloader, model, pretrain):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings_1 = np.zeros((len(dataloader.dataset), networks.vis_size))\n",
    "        embeddings_2 = np.zeros((len(dataloader.dataset), networks.vis_size))\n",
    "        labels = np.zeros(len(dataloader.dataset))\n",
    "        k = 0\n",
    "        for images, target in dataloader:\n",
    "            \n",
    "            images = images.cuda()\n",
    "            emb_1, emb_2= model.get_embedding(images, pretrain)\n",
    "            emb_1, emb_2 = emb_1.cpu(), emb_2.cpu()\n",
    "            embeddings_1[k:k+len(images)] = emb_1\n",
    "            embeddings_2[k:k+len(images)] = emb_2\n",
    "            labels[k:k+len(images)] = target.numpy()\n",
    "            k += len(images)\n",
    "    return embeddings_1, embeddings_2, labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "correct =0\n",
    "num_example =0\n",
    "test_loss_bce=0\n",
    "test_correct=0\n",
    "test_num_example =0\n",
    "for epoch in range(epochs_0):\n",
    "    model.train()\n",
    "    for idx, (images, target) in enumerate(train_loader):\n",
    "        images, target= images.cuda(), target.cuda()\n",
    "        out, rep_1, rep_2 = model(images, test= False)\n",
    "        loss_bce = criterion(out,target)\n",
    "        #loss_prox_1024 = criterion_prox_1024(rep_1, target) \n",
    "        #loss_conprox_1024 = criterion_conprox_1024(rep_1, target) \n",
    "        #loss_prox_256 = criterion_prox_256(rep_2, target) \n",
    "        #loss_conprox_256= criterion_conprox_256(rep_2, target) \n",
    "        loss = loss_bce #+ loss_prox_1024 + loss_prox_256 - loss_conprox_1024*0.0001 - loss_conprox_256*0.0001\n",
    "        preds = out.data.max(1, keepdim=True)[1]\n",
    "        correct += preds.eq(target.data.view_as(preds)).sum()\n",
    "        num_example += len(target)\n",
    "        optimizer_pre.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_pre.step()\n",
    "        \n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f}  Accuracy:  {}\".format(epoch+1,epochs_0, loss.item()/bs, correct.item()/num_example)\n",
    "        \n",
    "        \n",
    "        if idx % 500 == 0:\n",
    "            print(to_print)\n",
    "            \n",
    "            \n",
    "            \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, target in test_loader:\n",
    "            images, target = images.cuda(), target.cuda()\n",
    "            out, rep_1, rep_2= model(images, test=False)\n",
    "            loss_bce = criterion(out,target)\n",
    "            preds = out.data.max(1, keepdim=True)[1]\n",
    "            test_correct += preds.eq(target.data.view_as(preds)).sum()\n",
    "            test_num_example += len(target)\n",
    "            test_loss_bce+=loss_bce.item()\n",
    "            \n",
    "            \n",
    "            \n",
    "    test_loss_bce /= len(test_loader.dataset)\n",
    "    print( \"test_Loss: {:.3f} Test accuracy: {}\".format( test_loss_bce, test_correct.item()/test_num_example))\n",
    "    if epoch %10==0:\n",
    "        val_embeddings_1, val_embeddings_2, val_labels_baseline = extract_embeddings(test_loader, model,False)\n",
    "        plot_embeddings(val_embeddings_1, val_labels_baseline) \n",
    "        plot_embeddings(val_embeddings_2, val_labels_baseline)    \n",
    "        #fashion_scatter(t_sne_gen(rep_2.cpu()), target.cpu().numpy(),\"Clean_data: \"+\"VAE_\"+str(epoch)+\"softmax_rep2\",\"./softmaxreconstructed/\")  \n",
    "        #fashion_scatter(t_sne_gen(rep_1.cpu()), target.cpu().numpy(),\"Clean_data: \"+\"VAE_\"+str(epoch)+\"softmax_rep1\",\"./softmaxreconstructed/\")\n",
    "        attack_test(model, test_loader, nn.CrossEntropyLoss() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "correct =0\n",
    "num_example =0\n",
    "test_loss_bce=0\n",
    "test_correct=0\n",
    "test_num_example =0\n",
    "pre_wts= copy.deepcopy(list(model.embedding_net.layers.parameters()))\n",
    "for epoch in range(epochs_1):\n",
    "    model.train()\n",
    "    for idx, (images, target) in enumerate(train_loader):\n",
    "        images, target= images.cuda(), target.cuda()\n",
    "        out, rep_1, rep_2 = model(images,test=False)\n",
    "        #loss_bce = criterion(out,target)\n",
    "        loss_prox_1024, _ = criterion_prox_1024(rep_1, target) \n",
    "        loss_prox_256, preds = criterion_prox_256(rep_2, target) \n",
    "        loss = loss_prox_256 + loss_prox_1024 + 0.1 * cross_corr(criterion_prox_256.centers)\n",
    "        #preds = out.data.max(1, keepdim=True)[1]\n",
    "        correct += preds.eq(target.data.view_as(preds)).sum()\n",
    "        num_example += len(target)\n",
    "        optimizer_post.zero_grad()\n",
    "        optimizer_prox_1024.zero_grad() \n",
    "        optimizer_prox_256.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer_post.step()\n",
    "      \n",
    "        for param in criterion_prox_256.parameters():\n",
    "            param.grad.data *= (1. /1)\n",
    "        optimizer_prox_256.step()\n",
    "        \n",
    "        \n",
    "        for param in criterion_prox_1024.parameters():\n",
    "            param.grad.data *= (1. /1)\n",
    "        optimizer_prox_256.step()\n",
    "        \n",
    "    \n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f}  Accuracy:  {}\".format(epoch+1,epochs_1, loss.item()/bs, correct.item()/num_example)\n",
    "        \n",
    "        \n",
    "        if idx % 500 == 0:\n",
    "            print(to_print)\n",
    "            \n",
    "    #helper.plot_histogram(epoch,idx, pre_wts, list(model.embedding_net.layers.parameters()), list(gmm.parameters()), correct.item()/num_example,\"./figs/\")    \n",
    "            \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, target in test_loader:\n",
    "            images, target = images.cuda(), target.cuda()\n",
    "            out, rep_1, rep_2= model(images, test=False)\n",
    "            loss_bce = criterion(out,target)\n",
    "            loss_prox_256, preds = criterion_prox_256(rep_2, target) \n",
    "            #preds = out.data.max(1, keepdim=True)[1]\n",
    "            test_correct += preds.eq(target.data.view_as(preds)).sum()\n",
    "            test_num_example += len(target)\n",
    "            test_loss_bce+=loss_bce.item()\n",
    "            \n",
    "            \n",
    "            \n",
    "    test_loss_bce /= len(test_loader.dataset)\n",
    "    print( \"test_Loss: {:.3f} Test accuracy: {}\".format( test_loss_bce, test_correct.item()/test_num_example))\n",
    "    \n",
    "            \n",
    "    if epoch %10==0:\n",
    "        val_embeddings_1, val_embeddings_2, val_labels_baseline = extract_embeddings(test_loader, model,True)\n",
    "        plot_embeddings(val_embeddings_1, val_labels_baseline) \n",
    "        plot_embeddings(val_embeddings_2, val_labels_baseline)    \n",
    "        #fashion_scatter(t_sne_gen(rep_2.cpu()), target.cpu().numpy(),\"Clean_data: \"+\"VAE_\"+str(epoch)+\"softmax_rep2\",\"./softmaxreconstructed/\")  \n",
    "        #fashion_scatter(t_sne_gen(rep_1.cpu()), target.cpu().numpy(),\"Clean_data: \"+\"VAE_\"+str(epoch)+\"softmax_rep1\",\"./softmaxreconstructed/\")\n",
    "        attack_test(model, test_loader, nn.CrossEntropyLoss() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randint(0,10,(32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = torch.randn(10,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-b3ef995f8356>:2: UserWarning: This overload of addmm_ is deprecated:\n",
      "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  distmat.addmm_(1, -2, x, centers.t())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[123.4102, 143.3020, 156.8284, 110.1828, 126.2942, 132.7550, 111.2098,\n",
       "         124.4898, 103.6460, 116.6430],\n",
       "        [121.6430, 131.7764, 132.5055, 103.9737, 125.6361, 124.4557, 135.0880,\n",
       "         104.8350, 122.7714, 162.5762],\n",
       "        [119.0500, 113.2463, 109.8876,  85.4301, 104.2258, 135.8229, 119.5961,\n",
       "         104.2679,  97.2643, 110.8201],\n",
       "        [120.0528, 116.0704, 113.9804,  82.1713, 127.1796, 119.0071, 139.8723,\n",
       "         121.6481,  98.2589, 154.8297],\n",
       "        [118.7614, 130.8418, 138.5307,  89.2935, 144.2542, 141.4307, 114.3689,\n",
       "         125.0864, 101.4656, 175.1440],\n",
       "        [120.8643, 110.4047, 132.6652,  94.4043, 129.6920, 122.6486, 137.9121,\n",
       "         111.5099, 126.2253, 155.8465],\n",
       "        [118.5902, 129.5552, 128.4425, 101.3906, 113.5115, 102.6424, 128.7205,\n",
       "         104.5829, 108.3017, 150.3145],\n",
       "        [113.0652, 135.6908, 140.5896, 121.8736, 137.1870, 148.4487, 177.1949,\n",
       "         113.3142, 143.8784, 162.3724],\n",
       "        [141.4132, 145.1192, 159.5695,  93.8768, 142.6913, 136.5691, 125.8297,\n",
       "         123.9995, 132.3177, 132.4132],\n",
       "        [ 89.8529, 106.3034,  90.9741, 105.3470, 106.2699,  98.7801, 120.7019,\n",
       "          98.4176,  90.6191, 146.4630],\n",
       "        [129.5875, 115.5056, 127.2737, 109.1039, 146.1447, 138.4908, 151.1594,\n",
       "         140.9257, 111.0120, 161.0234],\n",
       "        [138.1160, 108.2330,  87.8248,  85.0440, 135.4546, 138.2607, 123.7576,\n",
       "         123.2076, 108.1329, 178.8613],\n",
       "        [123.4633, 121.0149, 134.8461,  95.3458, 125.7270, 157.9314, 122.1647,\n",
       "         145.3024, 114.3880, 140.2133],\n",
       "        [125.3083, 108.5066, 103.1491,  92.2139, 108.2240, 105.2757, 119.0541,\n",
       "         120.8167, 103.6994, 138.7343],\n",
       "        [133.0920,  97.9968, 131.9814,  92.0263, 140.9622, 123.0949, 114.4900,\n",
       "         130.2784,  88.8106, 114.4998],\n",
       "        [130.8047, 141.1052, 137.2785, 109.6985, 140.6809, 137.9347, 142.4931,\n",
       "         147.8445, 111.4834, 191.3335],\n",
       "        [104.6090, 149.0910, 157.8504, 113.0083, 148.0092, 133.7472, 164.5741,\n",
       "         142.4169, 118.1667, 188.4581],\n",
       "        [121.1356, 107.5724, 104.7335,  79.9326, 115.2795,  90.8499, 112.6588,\n",
       "         114.1467,  84.8028, 135.1698],\n",
       "        [116.9683, 143.6905, 123.2846,  92.5355, 109.5933, 116.8344, 177.6116,\n",
       "         121.4734, 120.9896, 177.1021],\n",
       "        [142.7330, 129.8658, 146.9490, 118.4707, 135.0716, 140.3526, 159.4150,\n",
       "         142.3679, 133.7868, 180.3205],\n",
       "        [140.1599, 180.5683, 180.2647, 172.2576, 143.9377, 154.6449, 186.8290,\n",
       "         121.9964, 183.5067, 191.9381],\n",
       "        [ 99.6837, 108.5712, 131.9895, 107.0379, 166.9887, 151.9276, 176.1650,\n",
       "         125.9874, 110.3012, 173.7402],\n",
       "        [130.0052, 142.5868, 149.5400, 144.2336, 131.0229,  97.2805, 123.5238,\n",
       "         139.1937, 123.0001, 172.1797],\n",
       "        [140.2064, 149.1181, 135.0334, 122.3159, 150.1938, 109.1951, 155.2051,\n",
       "         126.0367, 160.3044, 185.2823],\n",
       "        [104.6898, 124.2851,  99.1414,  93.0419, 106.9283,  96.3064, 139.7272,\n",
       "         127.5746,  98.4501, 149.4966],\n",
       "        [120.6777, 129.5388, 155.2897,  86.5334, 148.9408, 138.0153, 148.4540,\n",
       "         107.4342, 123.6286, 176.7944],\n",
       "        [128.9937, 152.7551, 151.5311, 106.9869, 121.2097, 155.9567, 183.3265,\n",
       "         115.1729,  99.8316, 194.6725],\n",
       "        [118.3199, 139.6728, 125.0420, 109.7496, 140.7763, 138.0088, 135.8832,\n",
       "         125.7026, 101.7535, 153.5181],\n",
       "        [117.0185, 118.3546, 143.7582,  91.3223, 135.8801, 120.2321,  99.0819,\n",
       "         103.0841, 111.4612, 161.2125],\n",
       "        [135.2901, 127.8162, 151.1840, 109.3886, 148.9641, 126.0979, 148.8174,\n",
       "         150.9770, 130.7260, 170.7999],\n",
       "        [113.7676, 124.3239, 127.9019, 100.6622, 125.9296, 125.7231, 145.8234,\n",
       "         118.3537,  92.2453, 148.6451],\n",
       "        [128.6354, 123.3611, 170.1613, 107.9222, 166.2225, 120.5189, 133.8028,\n",
       "         156.2133, 120.7437, 164.8488]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(32,10) + torch.pow(centers, 2).sum(dim=1, keepdim=True).expand(10, 32).t()\n",
    "distmat.addmm_(1, -2, x, centers.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.topk(distmat,2, dim=1, largest=False, sorted=True, out=None)\n",
    "d_1 = values[:,0]\n",
    "d_2 = values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=indices[:,0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=indices[:,1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[tuple(a) for a in zip(a,b)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 3),\n",
       " (0, 7),\n",
       " (0, 8),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 4),\n",
       " (3, 5),\n",
       " (3, 6),\n",
       " (3, 7),\n",
       " (3, 8),\n",
       " (5, 3),\n",
       " (5, 8),\n",
       " (7, 0),\n",
       " (8, 3)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[y==9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial until now:\n",
      " Value:  94.30999755859375\n",
      " Params: \n",
      "    post_epoch: 80\n",
      "    lr: 0.0005492907019496496\n",
      "    weight: 4.118563821233173e-05\n",
      "    lr_64: 0.0005125059487844361\n",
      "    lr_512: 0.00018328013361443643\n",
      "    margin_64: 0.8733892182380124\n",
      "    margin_512: 0.1392219583376752\n",
      "    sl_64: 0.23824426328723722\n",
      "    sl_512: 0.0442688718927673\n",
      "    lamda_64: 0.040272312116240425\n",
      "    lamda_512: 0.007846737960864792\n"
     ]
    }
   ],
   "source": [
    "study = joblib.load(\"./results/fmnist_optuna_bce.pkl\")\n",
    "print(\"Best trial until now:\")\n",
    "print(\" Value: \", study.best_trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
